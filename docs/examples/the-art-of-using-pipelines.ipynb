{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The art of using pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are a natural way to think about a machine learning system. Indeed with some practice a data scientist can visualise data \"flowing\" through a series of steps. The input is typically some raw data which has to be processed in some manner. The goal is to represent the data in such a way that is can be ingested by a machine learning algorithm. Along the way some steps will extract features, while others will normalize the data and remove undesirable elements. Pipelines are simple, and yet they are a powerful way of designing sophisticated machine learning systems.\n",
    "\n",
    "Both [scikit-learn](https://stackoverflow.com/questions/33091376/python-what-is-exactly-sklearn-pipeline-pipeline) and [pandas](https://tomaugspurger.github.io/method-chaining) make it possible to use pipelines. However it's quite rare to see pipelines being used in practice (at least on Kaggle). Sometimes you get to see people using scikit-learn's `pipeline` module, however the `pipe` method from `pandas` is sadly underappreciated. A big reason why pipelines are not given much love is that it's easier to think of batch learning in terms of a script or a notebook. Indeed many people doing data science seem to prefer a procedural style to a declarative style. Moreover in practice pipelines can be a bit rigid if one wishes to do non-orthodox operations.\n",
    "\n",
    "Although pipelines may be a bit of an odd fit for batch learning, they make complete sense when they are used for online learning. Indeed the UNIX philosophy has advocated the use of pipelines for data processing for many decades. If you can visualise data as a stream of observations then using pipelines should make a lot of sense to you. We'll attempt to convince you by writing a machine learning algorithm in a procedural way and then converting it to a declarative pipeline in small steps. Hopefully by the end you'll be convinced, or not!\n",
    "\n",
    "In this notebook we'll manipulate data from the [Kaggle Recruit Restaurants Visitor Forecasting competition](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting). The data is directly available through `river`'s `datasets` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:18:49.069231Z",
     "iopub.status.busy": "2020-11-25T21:18:49.068711Z",
     "iopub.status.idle": "2020-11-25T21:18:50.329174Z",
     "shell.execute_reply": "2020-11-25T21:18:50.328732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://maxhalford.github.io/files/datasets/kaggle_recruit_restaurants.zip (4.28 MB)\n",
      "Uncompressing into /home/runner/river_data/Restaurants\n",
      "{'area_name': 'Tōkyō-to Nerima-ku Toyotamakita',\n",
      " 'date': datetime.datetime(2016, 1, 1, 0, 0),\n",
      " 'genre_name': 'Izakaya',\n",
      " 'is_holiday': True,\n",
      " 'latitude': 35.7356234,\n",
      " 'longitude': 139.6516577,\n",
      " 'store_id': 'air_04341b588bde96cd'}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from river import datasets\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    pprint(x)\n",
    "    pprint(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by building and running a model using a procedural coding style. The performance of the model doesn't matter, we're simply interested in the design of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:18:50.369286Z",
     "iopub.status.busy": "2020-11-25T21:18:50.336179Z",
     "iopub.status.idle": "2020-11-25T21:19:11.236255Z",
     "shell.execute_reply": "2020-11-25T21:19:11.235740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.465114\n"
     ]
    }
   ],
   "source": [
    "from river import feature_extraction\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import preprocessing\n",
    "from river import stats\n",
    "\n",
    "\n",
    "means = (\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)),\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    \n",
    "    # Derive date features\n",
    "    x['weekday'] = x['date'].weekday()\n",
    "    x['is_weekend'] = x['date'].weekday() in (5, 6)\n",
    "    \n",
    "    # Process the rolling means of the target  \n",
    "    for mean in means:\n",
    "        x = {**x, **mean.transform_one(x)}\n",
    "        mean.learn_one(x, y)\n",
    "    \n",
    "    # Remove the key/value pairs that aren't features\n",
    "    for key in ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']:\n",
    "        x.pop(key)\n",
    "    \n",
    "    # Rescale the data\n",
    "    x = scaler.learn_one(x).transform_one(x)\n",
    "    \n",
    "    # Fit the linear regression\n",
    "    y_pred = lin_reg.predict_one(x)\n",
    "    lin_reg.learn_one(x, y)\n",
    "    \n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "    \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not using many features. We can print the last `x` to get an idea of the features (don't forget they've been scaled!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:19:11.240297Z",
     "iopub.status.busy": "2020-11-25T21:19:11.239486Z",
     "iopub.status.idle": "2020-11-25T21:19:11.243794Z",
     "shell.execute_reply": "2020-11-25T21:19:11.243321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_holiday': -0.23103573677646685,\n",
      " 'is_weekend': 1.6249280076334165,\n",
      " 'target_rollingmean_14_by_store_id': -1.4125913815779154,\n",
      " 'target_rollingmean_21_by_store_id': -1.3980979075298519,\n",
      " 'target_rollingmean_7_by_store_id': -1.3502314499809096,\n",
      " 'weekday': 1.0292832579142892}\n"
     ]
    }
   ],
   "source": [
    "pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chunk of code is quite explicit but it's a bit verbose. The whole point of libraries such as `river` is to make life easier for users. Moreover there's too much space for users to mess up the order in which things are done, which increases the chance of there being target leakage. We'll now rewrite our model in a declarative fashion using a pipeline *à la sklearn*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:19:11.263611Z",
     "iopub.status.busy": "2020-11-25T21:19:11.249886Z",
     "iopub.status.idle": "2020-11-25T21:19:42.977352Z",
     "shell.execute_reply": "2020-11-25T21:19:42.977722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 8.38533\n"
     ]
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "\n",
    "def get_date_features(x):\n",
    "    weekday =  x['date'].weekday()\n",
    "    return {'weekday': weekday, 'is_weekend': weekday in (5, 6)}\n",
    "\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "metric = metrics.MAE()\n",
    "\n",
    "for x, y in datasets.Restaurants():\n",
    "    \n",
    "    # Make a prediction without using the target\n",
    "    y_pred = model.predict_one(x)\n",
    "    \n",
    "    # Update the model using the target\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    # Update the metric using the out-of-fold prediction\n",
    "    metric.update(y, y_pred)\n",
    "    \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a `Pipeline` to arrange each step in a sequential order. A `TransformerUnion` is used to merge multiple feature extractors into a single transformer. The `for` loop is now much shorter and is thus easier to grok: we get the out-of-fold prediction, we fit the model, and finally we update the metric. This way of evaluating a model is typical of online learning, and so we put it wrapped it inside a function called `progressive_val_score` part of the `evaluate` module. We can use it to replace the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:19:42.989423Z",
     "iopub.status.busy": "2020-11-25T21:19:42.984233Z",
     "iopub.status.idle": "2020-11-25T21:20:21.386004Z",
     "shell.execute_reply": "2020-11-25T21:20:21.385532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import evaluate\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    ('features', compose.TransformerUnion(\n",
    "        ('date_features', compose.FuncTransformer(get_date_features)),\n",
    "        ('last_7_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7))),\n",
    "        ('last_14_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14))),\n",
    "        ('last_21_mean', feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)))\n",
    "    )),\n",
    "    ('drop_non_features', compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude')),\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LinearRegression())\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset=datasets.Restaurants(), model=model, metric=metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you couldn't have used the `progressive_val_score` method if you wrote the model in a procedural manner.\n",
    "\n",
    "Our code is getting shorter, but it's still a bit difficult on the eyes. Indeed there is a lot of boilerplate code associated with pipelines that can get tedious to write. However `river` has some special tricks up it's sleeve to save you from a lot of pain.\n",
    "\n",
    "The first trick is that the name of each step in the pipeline can be omitted. If no name is given for a step then `river` automatically infers one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:20:21.392283Z",
     "iopub.status.busy": "2020-11-25T21:20:21.391812Z",
     "iopub.status.idle": "2020-11-25T21:20:59.689323Z",
     "shell.execute_reply": "2020-11-25T21:20:59.688518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.TransformerUnion(\n",
    "        compose.FuncTransformer(get_date_features),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)),\n",
    "        feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    "    ),\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood a `Pipeline` inherits from `collections.OrderedDict`. Indeed this makes sense because if you think about it a `Pipeline` is simply a sequence of steps where each step has a name. The reason we mention this is because it means you can manipulate a `Pipeline` the same way you would manipulate an ordinary `dict`. For instance we can print the name of each step by using the `keys` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:20:59.693485Z",
     "iopub.status.busy": "2020-11-25T21:20:59.692293Z",
     "iopub.status.idle": "2020-11-25T21:20:59.697435Z",
     "shell.execute_reply": "2020-11-25T21:20:59.697019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerUnion\n",
      "Discard\n",
      "StandardScaler\n",
      "LinearRegression\n"
     ]
    }
   ],
   "source": [
    "for name in model.steps:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is a `FeatureUnion` and it's string representation contains the string representation of each of it's elements. Not having to write names saves up some time and space and is certainly less tedious.\n",
    "\n",
    "The next trick is that we can use mathematical operators to compose our pipeline. For example we can use the `+` operator to merge `Transformer`s into a `TransformerUnion`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:20:59.714373Z",
     "iopub.status.busy": "2020-11-25T21:20:59.703413Z",
     "iopub.status.idle": "2020-11-25T21:21:38.011759Z",
     "shell.execute_reply": "2020-11-25T21:21:38.011316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = compose.Pipeline(\n",
    "    compose.FuncTransformer(get_date_features) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)) + \\\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21)),\n",
    "\n",
    "    compose.Discard('store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude'),\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewhise we can use the `|` operator to assemble steps into a `Pipeline`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:21:38.024089Z",
     "iopub.status.busy": "2020-11-25T21:21:38.018029Z",
     "iopub.status.idle": "2020-11-25T21:22:16.517314Z",
     "shell.execute_reply": "2020-11-25T21:22:16.516850Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = (\n",
    "    compose.FuncTransformer(get_date_features) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(7)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(14)) +\n",
    "    feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(21))\n",
    ")\n",
    "\n",
    "to_discard = ['store_id', 'date', 'genre_name', 'area_name', 'latitude', 'longitude']\n",
    "\n",
    "model = model | compose.Discard(*to_discard) | preprocessing.StandardScaler()\n",
    "\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you'll agree that this is a powerful way to express machine learning pipelines. For some people this should be quite remeniscent of the UNIX pipe operator. One final trick we want to mention is that functions are automatically wrapped with a `FuncTransformer`, which can be quite handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:22:16.523381Z",
     "iopub.status.busy": "2020-11-25T21:22:16.522916Z",
     "iopub.status.idle": "2020-11-25T21:22:54.850491Z",
     "shell.execute_reply": "2020-11-25T21:22:54.850014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAE: 8.38533"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_date_features\n",
    "\n",
    "for n in [7, 14, 21]:\n",
    "    model += feature_extraction.TargetAgg(by='store_id', how=stats.RollingMean(n))\n",
    "\n",
    "model |= compose.Discard(*to_discard)\n",
    "model |= preprocessing.StandardScaler()\n",
    "model |= linear_model.LinearRegression()\n",
    "\n",
    "evaluate.progressive_val_score(datasets.Restaurants(), model, metrics.MAE())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally some may prefer the procedural style we first used because they find it easier to work with. It all depends on your style and you should use what you feel comfortable with. However we encourage you to use operators because we believe that this will increase the readability of your code, which is very important. To each their own!\n",
    "\n",
    "Before finishing we can take a look at what our pipeline looks graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-25T21:22:54.859314Z",
     "iopub.status.busy": "2020-11-25T21:22:54.858845Z",
     "iopub.status.idle": "2020-11-25T21:22:54.877415Z",
     "shell.execute_reply": "2020-11-25T21:22:54.877020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1021pt\" height=\"404pt\"\n",
       " viewBox=\"0.00 0.00 1020.93 404.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 400)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-400 1016.9314,-400 1016.9314,4 -4,4\"/>\n",
       "<!-- x -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"439.7935\" cy=\"-378\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.7935\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">x</text>\n",
       "</g>\n",
       "<!-- get_date_features -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>get_date_features</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"72.7935\" cy=\"-306\" rx=\"72.5877\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"72.7935\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">get_date_features</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;get_date_features -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x&#45;&gt;get_date_features</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M413.3198,-373.2943C362.7754,-364.2451 249.0736,-343.5764 153.7935,-324 148.1889,-322.8485 142.3736,-321.621 136.5479,-320.3688\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"137.2839,-316.9471 126.7691,-318.2472 135.7997,-323.788 137.2839,-316.9471\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_7_by_store_id -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>target_rollingmean_7_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"295.7935\" cy=\"-306\" rx=\"132.6765\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"295.7935\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target_rollingmean_7_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_7_by_store_id -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x&#45;&gt;target_rollingmean_7_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M417.923,-367.0647C397.3215,-356.764 365.8495,-341.028 340.0139,-328.1102\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"341.2907,-324.8355 330.7812,-323.4938 338.1602,-331.0965 341.2907,-324.8355\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_14_by_store_id -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>target_rollingmean_14_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"583.7935\" cy=\"-306\" rx=\"137.2758\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"583.7935\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target_rollingmean_14_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_14_by_store_id -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x&#45;&gt;target_rollingmean_14_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M461.664,-367.0647C482.2656,-356.764 513.7375,-341.028 539.5731,-328.1102\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"541.4269,-331.0965 548.8059,-323.4938 538.2964,-324.8355 541.4269,-331.0965\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_21_by_store_id -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>target_rollingmean_21_by_store_id</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"875.7935\" cy=\"-306\" rx=\"137.2758\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"875.7935\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">target_rollingmean_21_by_store_id</text>\n",
       "</g>\n",
       "<!-- x&#45;&gt;target_rollingmean_21_by_store_id -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x&#45;&gt;target_rollingmean_21_by_store_id</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M466.0854,-373.6582C526.8433,-363.6248 679.4902,-338.4171 780.3298,-321.7647\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"780.9299,-325.2131 790.2259,-320.1304 779.7893,-318.3066 780.9299,-325.2131\"/>\n",
       "</g>\n",
       "<!-- ~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"439.7935\" cy=\"-234\" rx=\"247.7561\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.7935\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</text>\n",
       "</g>\n",
       "<!-- get_date_features&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>get_date_features&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126.7691,-293.7528C135.7828,-291.7802 145.0449,-289.7975 153.7935,-288 214.4848,-275.5303 282.6505,-262.6173 337.2067,-252.5466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"338.0299,-255.9539 347.2303,-250.7 336.7616,-249.0697 338.0299,-255.9539\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_7_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>target_rollingmean_7_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M330.6519,-288.5708C350.0113,-278.8911 374.304,-266.7448 395.0531,-256.3702\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"396.6253,-259.4973 404.0043,-251.8946 393.4948,-253.2363 396.6253,-259.4973\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_14_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>target_rollingmean_14_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M548.9352,-288.5708C529.5758,-278.8911 505.2831,-266.7448 484.534,-256.3702\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"486.0923,-253.2363 475.5827,-251.8946 482.9618,-259.4973 486.0923,-253.2363\"/>\n",
       "</g>\n",
       "<!-- target_rollingmean_21_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;] -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>target_rollingmean_21_by_store_id&#45;&gt;~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M790.4339,-291.9039C721.6293,-280.5417 624.3868,-264.4833 549.7544,-252.1587\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"550.2798,-248.6981 539.8432,-250.522 549.1393,-255.6046 550.2798,-248.6981\"/>\n",
       "</g>\n",
       "<!-- StandardScaler -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>StandardScaler</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"439.7935\" cy=\"-162\" rx=\"63.8893\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.7935\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">StandardScaler</text>\n",
       "</g>\n",
       "<!-- ~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]&#45;&gt;StandardScaler -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>~[&#39;area_name&#39;, &#39;date&#39;, &#39;genre_name&#39;, &#39;latitude&#39;, &#39;longitude&#39;, &#39;store_id&#39;]&#45;&gt;StandardScaler</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M439.7935,-215.8314C439.7935,-208.131 439.7935,-198.9743 439.7935,-190.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.2936,-190.4132 439.7935,-180.4133 436.2936,-190.4133 443.2936,-190.4132\"/>\n",
       "</g>\n",
       "<!-- LinearRegression -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>LinearRegression</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"439.7935\" cy=\"-90\" rx=\"72.5877\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.7935\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">LinearRegression</text>\n",
       "</g>\n",
       "<!-- StandardScaler&#45;&gt;LinearRegression -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>StandardScaler&#45;&gt;LinearRegression</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M439.7935,-143.8314C439.7935,-136.131 439.7935,-126.9743 439.7935,-118.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.2936,-118.4132 439.7935,-108.4133 436.2936,-118.4133 443.2936,-118.4132\"/>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>y</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"439.7935\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.7935\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">y</text>\n",
       "</g>\n",
       "<!-- LinearRegression&#45;&gt;y -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>LinearRegression&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M439.7935,-71.8314C439.7935,-64.131 439.7935,-54.9743 439.7935,-46.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"443.2936,-46.4132 439.7935,-36.4133 436.2936,-46.4133 443.2936,-46.4132\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f6b33f46be0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
